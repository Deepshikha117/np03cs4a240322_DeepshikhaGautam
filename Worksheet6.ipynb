{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L4uTraslyj5Q"
      },
      "outputs": [],
      "source": [
        "#Task 1\n",
        "def logistic_function(x):\n",
        "    \"\"\"\n",
        "    Computes the logistic function applied to any value of x.\n",
        "    Arguments:\n",
        "        x: scalar or numpy array of any size.\n",
        "    Returns:\n",
        "        y: logistic function applied to x.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    y = 1 / (1 + np.exp(-x))\n",
        "    return y\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for logistic regression\n",
        "import numpy as np\n",
        "def test_logistic_function():\n",
        "  #Test with scalar input\n",
        "    x_scalar = 0\n",
        "    expected_output_scalar = round(1 / (1 + np.exp(0)), 3)  # Expected output: 0.5\n",
        "    assert round(logistic_function(x_scalar), 3) == expected_output_scalar, \"Test failed for scalar input\"\n",
        "#Test with positive scalar input\n",
        "    x_pos = 2\n",
        "    expected_output_pos = round(1 / (1 + np.exp(-2)), 3)  # Expected output: ~0.881\n",
        "    assert round(logistic_function(x_pos), 3) == expected_output_pos, \"Test failed for positive scalar input\"\n",
        "#Test with negative scalar input\n",
        "    x_neg = -3\n",
        "    expected_output_neg = round(1 / (1 + np.exp(3)), 3)  # Expected output: ~0.047\n",
        "    assert round(logistic_function(x_neg), 3) == expected_output_neg, \"Test failed for negative scalar input\"\n",
        "#Test with numpy array input\n",
        "    x_array = np.array([0, 2, -3])\n",
        "    expected_output_array = np.array([0.5, 0.881, 0.047])\n",
        "    assert np.all(np.round(logistic_function(x_array), 3) == expected_output_array), \"Test failed for numpy array input\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_logistic_function()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KWI9AdmPymEb",
        "outputId": "bafda2e0-ec2b-467d-aae8-2ee9681d7aef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 2\n",
        "def log_loss(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for true target value y (0 or 1) and predicted target value y' inbetween {0-1}.\n",
        "    Arguments:\n",
        "        y_true (scalar): true target value {0 or 1}.\n",
        "        y_pred (scalar): predicted target value {0-1}.\n",
        "    Returns:\n",
        "        loss (float): loss/error value\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    # Ensure y_pred is clipped to avoid log(0)\n",
        "    y_pred = np.clip(y_pred, 1e-10, 1 - 1e-10)\n",
        "    loss = - (y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
        "    return loss\n"
      ],
      "metadata": {
        "id": "kACugNyDymHQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for log loss\n",
        "import numpy as np\n",
        "\n",
        "def test_log_loss():\n",
        "    \"\"\"\n",
        "    Test cases for the log_loss function.\n",
        "    \"\"\"\n",
        "    # Test case 1: Perfect prediction (y_true = 1, y_pred = 1)\n",
        "    y_true = 1\n",
        "    y_pred = 1\n",
        "    expected_loss = 0.0\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss), \\\n",
        "        \"Test failed for perfect prediction (y_true=1, y_pred=1)\"\n",
        "\n",
        "    # Test case 2: Perfect prediction (y_true = 0, y_pred = 0)\n",
        "    y_true = 0\n",
        "    y_pred = 0\n",
        "    expected_loss = 0.0\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss), \\\n",
        "        \"Test failed for perfect prediction (y_true=0, y_pred=0)\"\n",
        "\n",
        "    # Test case 3: Incorrect prediction (y_true = 1, y_pred = 0) – should raise error before clipping,\n",
        "    # but your template clips y_pred, so log_loss will be very large instead of inf.\n",
        "    y_true = 1\n",
        "    y_pred = 0\n",
        "    try:\n",
        "        _ = log_loss(y_true, y_pred)\n",
        "    except ValueError:\n",
        "        pass  # If student had used raw log(0), this would be triggered\n",
        "\n",
        "    # Test case 4: Incorrect prediction (y_true = 0, y_pred = 1)\n",
        "    y_true = 0\n",
        "    y_pred = 1\n",
        "    try:\n",
        "        _ = log_loss(y_true, y_pred)\n",
        "    except ValueError:\n",
        "        pass\n",
        "\n",
        "    # Test case 5: Partially correct prediction (y_true = 1, y_pred = 0.8)\n",
        "    y_true = 1\n",
        "    y_pred = 0.8\n",
        "    expected_loss = -(1 * np.log(0.8) + (0 * np.log(0.2)))  # ~0.2231\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \\\n",
        "        \"Test failed for partially correct prediction (y_true=1, y_pred=0.8)\"\n",
        "\n",
        "    # Partially correct prediction (y_true = 0, y_pred = 0.2)\n",
        "    y_true = 0\n",
        "    y_pred = 0.2\n",
        "    expected_loss = -(0 * np.log(0.2) + (1 * np.log(0.8)))  # ~0.2231\n",
        "    assert np.isclose(log_loss(y_true, y_pred), expected_loss, atol=1e-6), \\\n",
        "        \"Test failed for partially correct prediction (y_true=0, y_pred=0.2)\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_log_loss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N3TH5hElymJz",
        "outputId": "bfc0997d-47cb-4a4c-c130-ce7626a9b29d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 3\n",
        "def cost_function(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes log loss for inputs true value (0 or 1) and predicted value (between 0 and 1)\n",
        "    Args:\n",
        "        y_true (array_like, shape (n,)): array of true values (0 or 1)\n",
        "        y_pred (array_like, shape (n,)): array of predicted values (probability of y_pred being 1)\n",
        "    Returns:\n",
        "        cost (float): nonnegative cost corresponding to y_true and y_pred\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    assert len(y_true) == len(y_pred), \"Length of true values and length of predicted values do not match\"\n",
        "\n",
        "    y_true = np.array(y_true)\n",
        "    y_pred = np.array(y_pred)\n",
        "    #vector of individual losses\n",
        "    loss_vec = np.array([log_loss(y_true[i], y_pred[i]) for i in range(len(y_true))])\n",
        "    #average loss\n",
        "    cost = np.mean(loss_vec)\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "nEaj2zOUymMf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for cost function\n",
        "import numpy as np\n",
        "\n",
        "def test_cost_function():\n",
        "    \"\"\"\n",
        "    Test cases for the cost_function.\n",
        "    \"\"\"\n",
        "    #Test case: simple example\n",
        "    y_true = np.array([1, 0, 1])\n",
        "    y_pred = np.array([0.9, 0.1, 0.8])\n",
        "\n",
        "    expected_cost = (\n",
        "        - (1 * np.log(0.9) + (0 * np.log(0.1))) +\n",
        "        - (0 * np.log(0.1) + (1 * np.log(0.9))) +\n",
        "        - (1 * np.log(0.8) + (0 * np.log(0.2)))\n",
        "    ) / 3\n",
        "\n",
        "    result = cost_function(y_true, y_pred)\n",
        "    assert np.isclose(result, expected_cost, atol=1e-6), \\\n",
        "        f\"Test failed: {result} != {expected_cost}\"\n",
        "\n",
        "    print(\"Test passed for simple case!\")\n",
        "\n",
        "test_cost_function()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vqqi9sdmymP8",
        "outputId": "4ef1b734-c59c-486d-8ca1-d5c9c1cf96df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed for simple case!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 4\n",
        "def costfunction_logreg(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes the cost function, given data and model parameters.\n",
        "    Args:\n",
        "        X (ndarray, shape (n,d)): data on features, n observations with d features.\n",
        "        y (array_like, shape (n,)): array of true values of target (0 or 1).\n",
        "        w (array_like, shape (d,)): weight parameters of the model.\n",
        "        b (float): bias parameter of the model.\n",
        "    Returns:\n",
        "        cost (float): nonnegative cost corresponding to y and y_pred.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n\n",
        "    assert len(w) == d\n",
        "\n",
        "    #Compute z using np.dot\n",
        "    z = np.dot(X, w) + b\n",
        "    #Compute predictions using logistic (sigmoid)\n",
        "    y_pred = logistic_function(z) # vector of probabilities\n",
        "    #Compute the cost using the cost function\n",
        "    cost = cost_function(y, y_pred)\n",
        "    return cost\n"
      ],
      "metadata": {
        "id": "pYCkIc3nymW-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test example\n",
        "import numpy as np\n",
        "X = np.array([[10, 20], [-10, 10]])\n",
        "y = np.array([1, 0])\n",
        "w = np.array([0.5, 1.5])\n",
        "b = 1\n",
        "print(f\"cost for logistic regression(X = {X}, y = {y}, w = {w}, b = {b}) = {costfunction_logreg(X, y, w, b)}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9DqT5mHYymZT",
        "outputId": "5bb782cb-459f-4dc7-c257-e74a0b5555df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cost for logistic regression(X = [[ 10  20]\n",
            " [-10  10]], y = [1 0], w = [0.5 1.5], b = 1) = 5.500008350834906\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 5\n",
        "def compute_gradient(X, y, w, b):\n",
        "    \"\"\"\n",
        "    Computes gradients of the cost function with respect to model parameters.\n",
        "    Args:\n",
        "        X (ndarray, shape (n,d)): Input data, n observations with d features\n",
        "        y (array_like, shape (n,)): True labels (0 or 1)\n",
        "        w (array_like, shape (d,)): Weight parameters of the model\n",
        "        b (float): Bias parameter of the model\n",
        "    Returns:\n",
        "        grad_w (array_like, shape (d,)): Gradients w.r.t weights\n",
        "        grad_b (float): Gradient w.r.t bias\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, f\"Expected y to have {n} elements, but got {len(y)}\"\n",
        "    assert len(w) == d, f\"Expected w to have {d} elements, but got {len(w)}\"\n",
        "\n",
        "#Compute predictions using logistic function (sigmoid)\n",
        "    z = np.dot(X, w) + b\n",
        "    y_pred = logistic_function(z)\n",
        "#Compute gradients\n",
        "    error = y_pred - y\n",
        "    grad_w = (1 / n) * np.dot(X.T, error)\n",
        "    grad_b = (1 / n) * np.sum(error)\n",
        "\n",
        "    return grad_w, grad_b\n"
      ],
      "metadata": {
        "id": "dLRfpe-Mymc1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for compute_gradient\n",
        "import numpy as np\n",
        "X = np.array([[0.1, 0.2], [-0.1, 0.1]])\n",
        "y = np.array([1, 0])\n",
        "w = np.array([0.5, 1.5])\n",
        "b = 1.0\n",
        "\n",
        "try:\n",
        "    grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "    print(\"Gradients computed successfully.\")\n",
        "    print(f\"grad_w: {grad_w}\")\n",
        "    print(f\"grad_b: {grad_b}\")\n",
        "except AssertionError as e:\n",
        "    print(f\"Assertion error: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMlHmI1r0BIC",
        "outputId": "9190a928-1b0c-4019-d483-7448652a59c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gradients computed successfully.\n",
            "grad_w: [-0.04780652  0.01692597]\n",
            "grad_b: 0.2721948668970852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 6\n",
        "def gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=True):\n",
        "    \"\"\"\n",
        "    Implements batch gradient descent to optimize logistic regression parameters.\n",
        "    Args:\n",
        "        X (ndarray, shape (n,d)): Data on features, n observations with d features\n",
        "        y (array_like, shape (n,)): True values of target (0 or 1)\n",
        "        w (array_like, shape (d,)): Initial weight parameters\n",
        "        b (float): Initial bias parameter\n",
        "        alpha (float): Learning rate\n",
        "        n_iter (int): Number of iterations\n",
        "        show_cost (bool): If True, displays cost every 100 iterations\n",
        "        show_params (bool): If True, displays parameters every 100 iterations\n",
        "    Returns:\n",
        "        w (array_like, shape (d,)): Optimized weight parameters\n",
        "        b (float): Optimized bias parameter\n",
        "        cost_history (list): List of cost values over iterations\n",
        "        params_history (list): List of parameters (w, b) over iterations\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "    n, d = X.shape\n",
        "    assert len(y) == n, \"Number of observations in X and y do not match\"\n",
        "    assert len(w) == d, \"Number of features in X and w do not match\"\n",
        "\n",
        "    cost_history = []\n",
        "    params_history = []\n",
        "\n",
        "    for i in range(n_iter):\n",
        "        # Compute gradients\n",
        "        grad_w, grad_b = compute_gradient(X, y, w, b)\n",
        "\n",
        "        # Update weights and bias\n",
        "        w = w - alpha * grad_w\n",
        "        b = b - alpha * grad_b\n",
        "\n",
        "        # Compute cost\n",
        "        cost = costfunction_logreg(X, y, w, b)\n",
        "\n",
        "        # Store cost and parameters\n",
        "        cost_history.append(cost)\n",
        "        params_history.append((w.copy(), b))\n",
        "\n",
        "        # Optionally print cost and parameters\n",
        "        if show_cost and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: Cost = {cost:.6f}\")\n",
        "        if show_params and (i % 100 == 0 or i == n_iter - 1):\n",
        "            print(f\"Iteration {i}: w = {w}, b = {b:.6f}\")\n",
        "\n",
        "    return w, b, cost_history, params_history\n"
      ],
      "metadata": {
        "id": "j9vP1QsW0BKe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for gradient_descent\n",
        "import numpy as np\n",
        "def test_gradient_descent():\n",
        "    X = np.array([[0.1, 0.2], [-0.1, 0.1]])\n",
        "    y = np.array([1, 0])\n",
        "    w = np.zeros(X.shape[1])\n",
        "    b = 0.0\n",
        "    alpha = 0.1\n",
        "    n_iter = 100\n",
        "\n",
        "    w_out, b_out, cost_history, _ = gradient_descent(X, y, w, b, alpha, n_iter, show_cost=False, show_params=False)\n",
        "\n",
        "    # Assertions\n",
        "    assert len(cost_history) == n_iter, \"Cost history length does not match the number of iterations\"\n",
        "    assert w_out.shape == w.shape, \"Shape of output weights does not match the initial weights\"\n",
        "    assert isinstance(b_out, float), \"Bias output is not a float\"\n",
        "    assert cost_history[-1] < cost_history[0], \"Cost did not decrease over iterations\"\n",
        "\n",
        "    print(\"All tests passed!\")\n",
        "\n",
        "test_gradient_descent()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GnslAFFP0BN3",
        "outputId": "0a16a31c-040a-4943-e08a-92f635bb5366"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 7\n",
        "def prediction(X, w, b, threshold=0.5):\n",
        "    \"\"\"\n",
        "    Predicts binary outcomes for given input features based on logistic regression parameters.\n",
        "    Arguments:\n",
        "        X (ndarray, shape (n,d)): Array of test independent variables (features) with n samples and d features.\n",
        "        w (ndarray, shape (d,)): Array of weights learned via gradient descent.\n",
        "        b (float): Bias learned via gradient descent.\n",
        "        threshold (float, optional): Classification threshold for predicting class labels. Default is 0.5.\n",
        "    Returns:\n",
        "        y_pred (ndarray, shape (n,)): Array of predicted dependent variable (binary class labels: 0 or 1).\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "#Compute the predicted probabilities using the logistic function\n",
        "    z = np.dot(X, w) + b\n",
        "    y_test_prob = logistic_function(z)\n",
        "#Classify based on the threshold\n",
        "    y_pred = (y_test_prob >= threshold).astype(int)\n",
        "    return y_pred\n"
      ],
      "metadata": {
        "id": "oJa9x6yR0ejo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Test for prediction\n",
        "import numpy as np\n",
        "\n",
        "def test_prediction():\n",
        "    X_test = np.array([[0.5, 1.0], [1.5, -0.5], [-0.5, -1.0]])\n",
        "    w_test = np.array([1.0, -1.0])\n",
        "    b_test = 0.0\n",
        "    threshold = 0.5\n",
        "\n",
        "    # Updated expected output (from your slide)\n",
        "    expected_output = np.array([0, 1, 1])\n",
        "\n",
        "    # Call the prediction function\n",
        "    y_pred = prediction(X_test, w_test, b_test, threshold)\n",
        "\n",
        "    # Assert that the output matches the expected output\n",
        "    assert np.array_equal(y_pred, expected_output), f\"Expected {expected_output}, but got {y_pred}\"\n",
        "    print(\"Test passed!\")\n",
        "\n",
        "test_prediction()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G3CPXWMC0emQ",
        "outputId": "76ef9fbf-391a-479b-ff4e-3b5e63d71f67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Task 8\n",
        "def evaluate_classification(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Computes the confusion matrix, precision, recall, and F1-score for binary classification.\n",
        "    Arguments:\n",
        "        y_true (ndarray, shape (n,)): Ground truth binary labels (0 or 1).\n",
        "        y_pred (ndarray, shape (n,)): Predicted binary labels (0 or 1).\n",
        "    Returns:\n",
        "        metrics (dict): A dictionary containing confusion matrix, precision, recall, and F1-score.\n",
        "    \"\"\"\n",
        "    import numpy as np\n",
        "\n",
        "    #Initialize confusion matrix components\n",
        "    TP = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    TN = np.sum((y_true == 0) & (y_pred == 0))\n",
        "    FP = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    FN = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    #Confusion matrix\n",
        "    confusion_matrix = np.array([[TN, FP],\n",
        "                                 [FN, TP]])\n",
        "    #Precision, recall, and F1-score\n",
        "    precision = TP / (TP + FP) if (TP + FP) > 0.0 else 0.0\n",
        "    recall = TP / (TP + FN) if (TP + FN) > 0.0 else 0.0\n",
        "    f1_score = (2 * precision * recall / (precision + recall)) if (precision + recall) > 0.0 else 0.0\n",
        "\n",
        "    #Metrics dictionary\n",
        "    metrics = {\n",
        "        \"confusion_matrix\": confusion_matrix,\n",
        "        \"precision\": precision,\n",
        "        \"recall\": recall,\n",
        "        \"f1_score\": f1_score\n",
        "    }\n",
        "    return metrics\n"
      ],
      "metadata": {
        "id": "fVxJQh0O0epy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this notebook, a complete logistic regression model for binary classification was implemented from scratch using only NumPy and related core libraries. Starting from the mathematical definition of the sigmoid function, helper functions were built and unit‑tested for the logistic (sigmoid) function, point‑wise log‑loss, and the average cost over a dataset. The cost was then re‑expressed in terms of model parameters\n",
        "w\n",
        "w and\n",
        "b\n",
        "b to obtain a vectorized logistic‑regression cost function, along with its gradients with respect to both weights and bias. Using these gradients, a batch gradient‑descent routine was implemented to iteratively update the parameters and minimize the cost, and convergence was verified by plotting cost versus iterations. Finally, the trained model was applied to the Pima‑Indians‑Diabetes dataset, predictions were generated using a probability threshold, and the classifier was evaluated using a confusion matrix, accuracy, precision, recall, and F1‑score, demonstrating the full pipeline from theory to implementation and evaluation."
      ],
      "metadata": {
        "id": "oIYruBKI1TFk"
      }
    }
  ]
}